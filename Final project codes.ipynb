{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "084_-97CV0T1"
   },
   "outputs": [],
   "source": [
    "#DECISION TREES\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel(r\"C:\\Users\\hasee\\OneDrive\\Desktop\\FDA Final Project\\fraud_detection_dataset3.csv\")\n",
    "\n",
    "# Drop irrelevant and high-correlation features\n",
    "drop_cols = [\n",
    "    'User_ID', 'Signup_Time', 'IP_Address', 'Device_Fingerprint',\n",
    "    'VPN_Used', 'Failed_Attempts', 'Time_Taken_To_Signup',\n",
    "    'Typing_Speed', 'Geolocation_Mismatch', 'ID_Verification_Time'\n",
    "]\n",
    "df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "\n",
    "# Convert binary Yes/No columns to 1/0\n",
    "binary_cols = ['Name_Matches_ID', 'Phone_Number_Reuse', 'Social_Media_Linked',\n",
    "               'Is_CNIC_Verified_With_NADRA', 'Is_SIM_Active',\n",
    "               'Accounts_From_Same_Device', 'All_Documents_Uploaded',\n",
    "               'Social_Media_Verification', 'Historical_Fraud',\n",
    "               'Email_Verified', 'Phone_Verified', 'Browser_Language_Mismatch']\n",
    "\n",
    "for col in binary_cols:\n",
    "    df[col] = df[col].map({'Yes': 1, 'No': 0, 'True': 1, 'False': 0})\n",
    "    df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "# One-hot encode Email_Domain\n",
    "df = pd.get_dummies(df, columns=['Email_Domain'], drop_first=True)\n",
    "\n",
    "# Drop remaining NA rows\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define features and label\n",
    "X = df.drop(columns=['Is_Fraud'])\n",
    "y = df['Is_Fraud']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train Decision Tree\n",
    "clf = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "clf.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df1tGulvWSzL"
   },
   "outputs": [],
   "source": [
    "#CATBOOST\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel(r\"C:\\Users\\hasee\\OneDrive\\Desktop\\FDA Final Project\\fraud_detection_dataset3.csv\")\n",
    "\n",
    "# Drop User_ID (not useful for prediction)\n",
    "df = df.drop(columns=['User_ID'])\n",
    "\n",
    "# List of binary columns (Yes/No to 1/0)\n",
    "binary_cols = [\n",
    "    'Name_Matches_ID', 'Phone_Number_Reuse', 'Social_Media_Linked',\n",
    "    'Geolocation_Mismatch', 'VPN_Used', 'Is_CNIC_Verified_With_NADRA',\n",
    "    'Is_SIM_Active', 'Accounts_From_Same_Device', 'All_Documents_Uploaded',\n",
    "    'Social_Media_Verification', 'Email_Verified', 'Phone_Verified',\n",
    "    'Browser_Language_Mismatch', 'Historical_Fraud'\n",
    "]\n",
    "\n",
    "# Convert binary columns to 1/0\n",
    "for col in binary_cols:\n",
    "    df[col] = df[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Define target and features\n",
    "y = df['Is_Fraud']\n",
    "X = df.drop(columns=['Is_Fraud'])\n",
    "\n",
    "# List of categorical columns that should be handled by CatBoost\n",
    "categorical_cols = ['IP_Address', 'Device_Fingerprint', 'Email_Domain']\n",
    "\n",
    "# Get indices of categorical columns\n",
    "cat_feature_indices = [X.columns.get_loc(col) for col in categorical_cols]\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train CatBoost model\n",
    "model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "model.fit(X_train, y_train, cat_features=cat_feature_indices)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"AUC-ROC Score:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance = model.get_feature_importance()\n",
    "feature_names = X.columns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_names, feature_importance)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('CatBoost Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f'CatBoost (AUC = {roc_auc_score(y_test, y_proba):.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPb9eCMkWcEr"
   },
   "outputs": [],
   "source": [
    "#CATBOOST WITH CLASS WEIGHTS\n",
    "\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel(r\"C:\\Users\\hasee\\OneDrive\\Desktop\\FDA Final Project\\fraud_detection_dataset3.csv\")\n",
    "\n",
    "# Drop columns you don't want\n",
    "df.drop(columns=['IP_Address', 'Device_Fingerprint', 'Signup_Time'], errors='ignore', inplace=True)\n",
    "\n",
    "# Auto-detect categorical columns (non-numeric)\n",
    "categorical_cols = df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "categorical_cols = [col for col in categorical_cols if col != 'Is_Fraud']\n",
    "\n",
    "# Convert to string (required by CatBoost)\n",
    "df[categorical_cols] = df[categorical_cols].astype(str)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns='Is_Fraud')\n",
    "y = df['Is_Fraud']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Create Pools\n",
    "train_pool = Pool(X_train, y_train, cat_features=categorical_cols)\n",
    "test_pool = Pool(X_test, cat_features=categorical_cols)\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# Train model\n",
    "model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    class_weights=class_weights,  # Pass the computed dictionary here\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "model.fit(train_pool)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"AUC-ROC Score: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vt-Gb1XSWiSw"
   },
   "outputs": [],
   "source": [
    "#EXTRA TREES CLASSIFIER\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_excel(r\"C:\\Users\\hasee\\OneDrive\\Desktop\\FDA Final Project\\fraud_detection_dataset3.csv\")\n",
    "\n",
    "# Drop non-numeric or high-cardinality columns\n",
    "columns_to_drop = ['User_ID', 'IP_Address', 'Device_Fingerprint', 'Signup_Time']\n",
    "df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "# Convert binary 'Yes'/'No' to 1/0\n",
    "binary_cols = [\n",
    "    'Name_Matches_ID', 'Phone_Number_Reuse', 'Social_Media_Linked',\n",
    "    'Geolocation_Mismatch', 'VPN_Used', 'Is_CNIC_Verified_With_NADRA',\n",
    "    'Is_SIM_Active', 'Accounts_From_Same_Device', 'All_Documents_Uploaded',\n",
    "    'Social_Media_Verification', 'Email_Verified', 'Phone_Verified',\n",
    "    'Browser_Language_Mismatch', 'Historical_Fraud'\n",
    "]\n",
    "\n",
    "for col in binary_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Define target and features\n",
    "target = 'Is_Fraud'\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Label encode categorical columns like Email_Domain\n",
    "categorical_cols = ['Email_Domain']\n",
    "X_encoded = X.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in X_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Handle missing values (avoids column mismatch error)\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "X_imputed_array = imputer.fit_transform(X_encoded)\n",
    "\n",
    "# Fix column name mismatch by using only columns that remain\n",
    "X_imputed = pd.DataFrame(X_imputed_array, columns=X_encoded.columns[:X_imputed_array.shape[1]])\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_imputed, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize Extra Trees Classifier\n",
    "model = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"AUC-ROC Score:\", roc_auc_score(y_test, y_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GipoZx01WoXf"
   },
   "outputs": [],
   "source": [
    "# K-MEANS\n",
    "\n",
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\"Load and Clean Data as the dataset is read from a CSV file and irrelevant or identifier columns such as `User_ID`, `Device_Fingerprint`, `IP_Address`, and `Signup_Time` are dropped to avoid data leakage.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 2. Load Data\n",
    "df = pd.read_csv(r\"C:\\Users\\hasee\\OneDrive\\Desktop\\FDA Final Project\\fraud_detection_dataset3.csv\")\n",
    "df = df.drop(columns=['User_ID', 'Device_Fingerprint', 'IP_Address', 'Signup_Time'])  # Drop identifiers\n",
    "\n",
    "\"\"\"Feature Encoding categorical variables are converted into numerical format using one-hot encoding (`get_dummies`) except the target variable `Is_Fraud`.\n",
    "\n",
    "Feature Scaling features are standardized using `StandardScaler` to ensure equal weightage during clustering.\n",
    "\"\"\"\n",
    "\n",
    "# 3. Encode categorical features\n",
    "df_encoded = pd.get_dummies(df.drop(columns=['Is_Fraud']), drop_first=True)\n",
    "\n",
    "# 4. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_encoded)\n",
    "\n",
    "\"\"\"Apply K-Means Clustering K-Means is initialized with 2 clusters (fraud vs non-fraud) and applied to the scaled data. Cluster labels are added to the DataFrame.\"\"\"\n",
    "\n",
    "# 5. K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "\"\"\"Visualize Clusters PCA (Principal Component Analysis) is used to reduce dimensionality to 2D for visualization. A scatter plot shows the clustering results in a 2D plane.\"\"\"\n",
    "\n",
    "# 6. Visualize Clusters with PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=pca_components[:, 0], y=pca_components[:, 1], hue=df['Cluster'], palette='Set2')\n",
    "plt.title(\"K-Means Clustering (PCA 2D Projection)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Map Cluster Labels to Match Actual Fraud Labels since K-Means cluster labels are arbitrary, they are mapped to match actual labels (`Is_Fraud`) based on the most frequent class in each cluster using `scipy.stats.mode`.\"\"\"\n",
    "\n",
    "# Compare predicted clusters to actual fraud\n",
    "# Remap cluster labels to match fraud labels better (optional)\n",
    "from scipy.stats import mode\n",
    "\n",
    "mapping = {}\n",
    "for cluster_label in df['Cluster'].unique():\n",
    "    # Use .mode and .iloc[0] for safety\n",
    "    common_class = mode(df[df['Cluster'] == cluster_label]['Is_Fraud'], keepdims=True).mode[0]\n",
    "    mapping[cluster_label] = common_class\n",
    "\n",
    "\n",
    "df['Cluster_Mapped'] = df['Cluster'].map(mapping)\n",
    "\n",
    "\"\"\"Evaluation performance is evaluated using a confusion matrix, classification report (precision, recall, F1-score), and ROC AUC score to assess how well clustering aligns with actual fraud labels.\"\"\"\n",
    "\n",
    "# Evaluation\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(df['Is_Fraud'], df['Cluster_Mapped']))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(df['Is_Fraud'], df['Cluster_Mapped']))\n",
    "\n",
    "print(\"\\nAUC-ROC Score:\")\n",
    "print(roc_auc_score(df['Is_Fraud'], df['Cluster_Mapped']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y94E1v6AWvaa"
   },
   "outputs": [],
   "source": [
    "#LIGHTGBM\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\hasee\\OneDrive\\Desktop\\FDA Final Project\\fraud_detection_dataset3.csv\")\n",
    "df = df.dropna(subset=['Is_Fraud'])\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['User_ID', 'Is_Fraud'])\n",
    "y = df['Is_Fraud']\n",
    "\n",
    "# Binary mapping\n",
    "binary_map = {'Yes': 1, 'No': 0}\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object' and set(X[col].dropna().unique()).issubset({'Yes', 'No'}):\n",
    "        X[col] = X[col].map(binary_map)\n",
    "\n",
    "# Identify column types\n",
    "cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), num_cols),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]), cat_cols)\n",
    "])\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "# Define and train LightGBM\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_alpha=0.3,\n",
    "    reg_lambda=1.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Evaluate\n",
    "val_preds = lgbm_model.predict(X_val_processed)\n",
    "print(f\"LightGBM Accuracy: {accuracy_score(y_val, val_preds):.4f}\")\n",
    "print(f\"LightGBM F1 Score: {f1_score(y_val, val_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUnFaKO5W2OG"
   },
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "\n",
    "# Install imbalanced-learn if not already installed\n",
    "!pip install -q imbalanced-learn\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    roc_curve, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, RocCurveDisplay # Import RocCurveDisplay here\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_excel(r\"C:\\Users\\hasee\\OneDrive\\Desktop\\FDA Final Project\\fraud_detection_dataset3.csv\")  # Replace with your actual file name\n",
    "\n",
    "# Display basic info\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop datetime columns to avoid type errors\n",
    "datetime_cols = df.select_dtypes(include=['datetime64', 'datetime64[ns]']).columns\n",
    "df = df.drop(columns=datetime_cols)\n",
    "\n",
    "# Print dropped datetime columns (if any)\n",
    "if len(datetime_cols) > 0:\n",
    "    print(\"Dropped datetime columns:\", list(datetime_cols))\n",
    "\n",
    "# Feature Engineering\n",
    "df['Contribution_Income_Ratio'] = df['Committee_Contribution_Amount'] / (df['Income'] + 1)\n",
    "df['Payout_Income_Ratio'] = df['Committee_Payout_Amount'] / (df['Income'] + 1)\n",
    "df['Net_Committee_Profit'] = df['Committee_Payout_Amount'] - df['Committee_Contribution_Amount']\n",
    "df['Committee_ROI'] = df['Net_Committee_Profit'] / (df['Committee_Contribution_Amount'] + 1)\n",
    "\n",
    "\n",
    "# Encode categorical features using LabelEncoder\n",
    "label_encoders = {}\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['Identity_theft'])\n",
    "y = df['Identity_theft']\n",
    "\n",
    "\n",
    "#Applying SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "RocCurveDisplay.from_estimator(rf_model, X_test, y_test)\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()\n",
    "\n",
    "# Feature importances\n",
    "importances = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "importances.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('Feature Importances from Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6e5s_EeXOLF"
   },
   "outputs": [],
   "source": [
    "#STACKED\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\hasee\\OneDrive\\Desktop\\FDA Final Project\\fraud_detection_dataset3.csv\")\n",
    "\n",
    "# Drop rows with missing target\n",
    "df = df.dropna(subset=['Is_Fraud'])\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['User_ID', 'Is_Fraud'])\n",
    "y = df['Is_Fraud']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "binary_map = {'Yes': 1, 'No': 0}\n",
    "\n",
    "# Map binary strings to integers where possible\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object' and set(X[col].dropna().unique()).issubset({'Yes', 'No'}):\n",
    "        X[col] = X[col].map(binary_map)\n",
    "\n",
    "# After binary mapping, identify remaining categorical and numeric columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), num_cols),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]), cat_cols)\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Fit-transform the data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "# Define base models\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=350,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=0.3,\n",
    "    reg_lambda=1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=350,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.3,\n",
    "    reg_lambda=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Final estimator\n",
    "meta_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Stacking model\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb),\n",
    "        ('lgbm', lgbm),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    stack_method='predict_proba',\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# Train the stacked model\n",
    "stacked_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Validation\n",
    "val_preds = stacked_model.predict(X_val_processed)\n",
    "accuracy = accuracy_score(y_val, val_preds)\n",
    "f1 = f1_score(y_val, val_preds)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation F1 Score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNlWWy05Y18v"
   },
   "outputs": [],
   "source": [
    "#NAIVE BAYES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix,\n",
    "    roc_curve, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\hasee\\OneDrive\\Desktop\\FDA Final Project\\fraud_detection_dataset3.csv\")\n",
    "\n",
    "# Drop identifier columns\n",
    "df = df.drop(columns=['User_ID', 'IP_Address', 'Device_Fingerprint', 'Signup_Time'])\n",
    "df.head()\n",
    "\n",
    "# Encode binary categorical features\n",
    "binary_map = {'Yes': 1, 'No': 0}\n",
    "for col in df.columns:\n",
    "    if set(df[col].unique()) <= {'Yes', 'No'}:\n",
    "        df[col] = df[col].map(binary_map)\n",
    "\n",
    "# Encode other categorical variables\n",
    "label_encoders = {}\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Handle missing values\n",
    "df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(columns='Is_Fraud')\n",
    "y = df['Is_Fraud']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(score_func=mutual_info_classif, k=10))  # You can tune k\n",
    "])\n",
    "\n",
    "# Define classifiers\n",
    "models = {\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix - {name}\")\n",
    "    plt.show()\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
