{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPm_olPZB8mu",
    "outputId": "234e98d7-75a2-49b1-b209-fd2f40e2ceda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble model...\n",
      "Validation Accuracy: 0.997750\n",
      "Cross-Validation Accuracy: 0.997470 (+/- 0.000159)\n",
      "\n",
      "Submission file 'submission_fda_improved fda_31.csv' has been created.\n",
      "Submission head:\n",
      "       ID         Y\n",
      "0  200001  0.000505\n",
      "1  200002  0.001326\n",
      "2  200003  0.000381\n",
      "3  200004  0.000387\n",
      "4  200005  0.000789\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load the competition data\"\"\"\n",
    "    train_df = pd.read_csv(r\"fda_trainingset.csv\")\n",
    "    test_df = pd.read_csv(r\"fda_testset.csv\")\n",
    "    return train_df, test_df\n",
    "\n",
    "def enhanced_feature_engineering(df, is_train=True, numeric_cols=None, top_features=None):\n",
    "    \"\"\"Add advanced feature engineering\"\"\"\n",
    "    if is_train:\n",
    "        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        if 'ID' in numeric_cols:\n",
    "            numeric_cols.remove('ID')\n",
    "        if 'Y' in numeric_cols:\n",
    "            numeric_cols.remove('Y')\n",
    "\n",
    "    # Interaction features\n",
    "    for i, col1 in enumerate(numeric_cols[:10]):\n",
    "        for col2 in numeric_cols[i+1:i+5]:\n",
    "            df[f'{col1}_x_{col2}'] = df[col1] * df[col2]\n",
    "            df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-6)\n",
    "\n",
    "    # Statistical features\n",
    "    df['row_sum'] = df[numeric_cols].sum(axis=1)\n",
    "    df['row_mean'] = df[numeric_cols].mean(axis=1)\n",
    "    df['row_std'] = df[numeric_cols].std(axis=1)\n",
    "\n",
    "    # Polynomial features for top 5 numeric columns\n",
    "    if len(numeric_cols) >= 5:\n",
    "        poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        df[numeric_cols[:5]] = imputer.fit_transform(df[numeric_cols[:5]])\n",
    "        poly_features = poly.fit_transform(df[numeric_cols[:5]])\n",
    "        poly_feature_names = poly.get_feature_names_out(numeric_cols[:5])\n",
    "        poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=df.index)\n",
    "        df = pd.concat([df, poly_df], axis=1)\n",
    "\n",
    "    # Feature importance-based interactions (if top_features provided)\n",
    "    if top_features and len(top_features) >= 2:\n",
    "        for i, col1 in enumerate(top_features[:2]):\n",
    "            for col2 in top_features[i+1:i+3]:\n",
    "                df[f'imp_{col1}_x_{col2}'] = df[col1] * df[col2]\n",
    "\n",
    "    return df, numeric_cols\n",
    "\n",
    "def preprocess_data(train_df, test_df):\n",
    "    \"\"\"Preprocess data including encoding and feature engineering\"\"\"\n",
    "    X = train_df.drop(columns=['ID', 'Y'])\n",
    "    y = train_df['Y']\n",
    "    test_ids = test_df['ID']\n",
    "    X_test = test_df.drop(columns=['ID'])\n",
    "\n",
    "    # Convert to float32 to save memory\n",
    "    X = X.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    for column in X.select_dtypes(include=['object']).columns:\n",
    "        le = LabelEncoder()\n",
    "        X[column] = le.fit_transform(X[column].astype(str))\n",
    "        X_test[column] = le.transform(X_test[column].astype(str))\n",
    "        label_encoders[column] = le\n",
    "\n",
    "    # Initial feature engineering to get feature importance\n",
    "    temp_pipeline = make_pipeline(SimpleImputer(strategy='median'), StandardScaler(), XGBClassifier(random_state=42))\n",
    "    temp_pipeline.fit(X, y)\n",
    "    feature_importance = pd.Series(temp_pipeline.named_steps['xgbclassifier'].feature_importances_, index=X.columns)\n",
    "    top_features = feature_importance.nlargest(5).index.tolist()\n",
    "\n",
    "    # Full feature engineering\n",
    "    X, numeric_cols = enhanced_feature_engineering(X, is_train=True, top_features=top_features)\n",
    "    X_test, _ = enhanced_feature_engineering(X_test, is_train=False, numeric_cols=numeric_cols, top_features=top_features)\n",
    "\n",
    "    return X, y, X_test, test_ids, top_features\n",
    "\n",
    "def build_ensemble_model():\n",
    "    \"\"\"Create a simplified ensemble of XGBoost and LightGBM\"\"\"\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=350,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=4,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=1.2  # Adjusted for class imbalance\n",
    "    )\n",
    "    lgbm = LGBMClassifier(\n",
    "        n_estimators=350,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=4,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        scale_pos_weight=1.2\n",
    "    )\n",
    "    return VotingClassifier(\n",
    "        estimators=[('xgb', xgb), ('lgbm', lgbm)],\n",
    "        voting='soft',\n",
    "        weights=[0.6, 0.4]  # Favor XGBoost slightly based on performance\n",
    "    )\n",
    "\n",
    "def optimize_threshold(y_true, y_prob):\n",
    "    \"\"\"Optimize classification threshold for maximum accuracy\"\"\"\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    best_threshold = 0.5\n",
    "    best_score = 0\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        score = accuracy_score(y_true, y_pred)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = threshold\n",
    "    return best_threshold\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    train_df, test_df = load_data()\n",
    "\n",
    "    # Preprocess data\n",
    "    X, y, X_test, test_ids, top_features = preprocess_data(train_df, test_df)\n",
    "\n",
    "    # Feature selection with RFE\n",
    "    base_model = XGBClassifier(random_state=42)\n",
    "    rfe = RFE(estimator=base_model, n_features_to_select=35)\n",
    "    X = pd.DataFrame(rfe.fit_transform(X, y), columns=X.columns[rfe.support_], index=X.index)\n",
    "    X_test = pd.DataFrame(rfe.transform(X_test), columns=X_test.columns[rfe.support_], index=X_test.index)\n",
    "\n",
    "    # Split data for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train ensemble model\n",
    "    print(\"Training ensemble model...\")\n",
    "    model = make_pipeline(\n",
    "        SimpleImputer(strategy='median'),\n",
    "        StandardScaler(),\n",
    "        build_ensemble_model()\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Validate model\n",
    "    val_probs = model.predict_proba(X_val)[:, 1]\n",
    "    threshold = optimize_threshold(y_val, val_probs)\n",
    "    val_preds = (val_probs >= threshold).astype(int)\n",
    "    val_score = accuracy_score(y_val, val_preds)\n",
    "    print(f\"Validation Accuracy: {val_score:.6f}\")\n",
    "\n",
    "    # Cross-validation with StratifiedKFold\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    print(f\"Cross-Validation Accuracy: {cv_scores.mean():.6f} (+/- {cv_scores.std() * 2:.6f})\")\n",
    "\n",
    "    # Generate predictions\n",
    "    test_probs = model.predict_proba(X_test)[:, 1]\n",
    "    test_preds = (test_probs >= threshold).astype(int)\n",
    "\n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({'ID': test_ids, 'Y': test_probs})\n",
    "    submission.to_csv('submission_fda_improved fda_31.csv', index=False)\n",
    "    print(\"\\nSubmission file 'submission_fda_improved fda_31.csv' has been created.\")\n",
    "    print(\"Submission head:\")\n",
    "    print(submission.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load the competition data\"\"\"\n",
    "    train_df = pd.read_csv(r\"fda_trainingset.csv\")\n",
    "    test_df = pd.read_csv(r\"fda_testset.csv\")\n",
    "    return train_df, test_df\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Loading the dataset and importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_feature_engineering(df, is_train=True, numeric_cols=None, top_features=None):\n",
    "    \"\"\"Add advanced feature engineering\"\"\"\n",
    "    if is_train:\n",
    "        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        if 'ID' in numeric_cols:\n",
    "            numeric_cols.remove('ID')\n",
    "        if 'Y' in numeric_cols:\n",
    "            numeric_cols.remove('Y')\n",
    "\n",
    "    # Interaction features\n",
    "    for i, col1 in enumerate(numeric_cols[:10]):\n",
    "        for col2 in numeric_cols[i+1:i+5]:\n",
    "            df[f'{col1}_x_{col2}'] = df[col1] * df[col2]\n",
    "            df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-6)\n",
    "\n",
    "    # Statistical features\n",
    "    df['row_sum'] = df[numeric_cols].sum(axis=1)\n",
    "    df['row_mean'] = df[numeric_cols].mean(axis=1)\n",
    "    df['row_std'] = df[numeric_cols].std(axis=1)\n",
    "\n",
    "    # Polynomial features for top 5 numeric columns\n",
    "    if len(numeric_cols) >= 5:\n",
    "        poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        df[numeric_cols[:5]] = imputer.fit_transform(df[numeric_cols[:5]])\n",
    "        poly_features = poly.fit_transform(df[numeric_cols[:5]])\n",
    "        poly_feature_names = poly.get_feature_names_out(numeric_cols[:5])\n",
    "        poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=df.index)\n",
    "        df = pd.concat([df, poly_df], axis=1)\n",
    "\n",
    "    # Feature importance-based interactions (if top_features provided)\n",
    "    if top_features and len(top_features) >= 2:\n",
    "        for i, col1 in enumerate(top_features[:2]):\n",
    "            for col2 in top_features[i+1:i+3]:\n",
    "                df[f'imp_{col1}_x_{col2}'] = df[col1] * df[col2]\n",
    "\n",
    "    return df, numeric_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering:\n",
    ". For the top 10 numeric columns, creates pairwise interaction features (multiplication and division) with the next 4 columns.\n",
    ". For the top 5 numeric columns, imputes missing values (median strategy) and creates polynomial features (degree 2, including squares and interactions) using PolynomialFeatures.\n",
    ". If top_features (from XGBoost importance) is provided, creates interaction features (multiplications) for the top 2 features paired with the next 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!-- def preprocess_data(train_df, test_df):\n",
    "    \"\"\"Preprocess data including encoding and feature engineering\"\"\"\n",
    "    X = train_df.drop(columns=['ID', 'Y'])\n",
    "    y = train_df['Y']\n",
    "    test_ids = test_df['ID']\n",
    "    X_test = test_df.drop(columns=['ID'])\n",
    "\n",
    "    # Convert to float32 to save memory\n",
    "    X = X.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    for column in X.select_dtypes(include=['object']).columns:\n",
    "        le = LabelEncoder()\n",
    "        X[column] = le.fit_transform(X[column].astype(str))\n",
    "        X_test[column] = le.transform(X_test[column].astype(str))\n",
    "        label_encoders[column] = le\n",
    "\n",
    "    # Initial feature engineering to get feature importance\n",
    "    temp_pipeline = make_pipeline(SimpleImputer(strategy='median'), StandardScaler(), XGBClassifier(random_state=42))\n",
    "    temp_pipeline.fit(X, y)\n",
    "    feature_importance = pd.Series(temp_pipeline.named_steps['xgbclassifier'].feature_importances_, index=X.columns)\n",
    "    top_features = feature_importance.nlargest(5).index.tolist()\n",
    "\n",
    "    # Full feature engineering\n",
    "    X, numeric_cols = enhanced_feature_engineering(X, is_train=True, top_features=top_features)\n",
    "    X_test, _ = enhanced_feature_engineering(X_test, is_train=False, numeric_cols=numeric_cols, top_features=top_features)\n",
    "\n",
    "    return X, y, X_test, test_ids, top_features\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre processing:\n",
    ". Seperate fetures and target drops ID and Y from train_df to create X (features) and extracts y (target).\n",
    ". Converts X and X_test to float32 to reduce memory usage.\n",
    ". Encode Categorical Variables identifies categorical columns (type object) and applies LabelEncoder to convert them to numeric values.\n",
    ". Uses a temporary pipeline (imputation, scaling, XGBoost) to fit the training data and extract feature importances and identifies the top 5 features (top_features) for use in feature engineering.\n",
    ". Applies enhanced_feature_engineering to both X (training) and X_test (test), passing top_features for additional interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!-- def build_ensemble_model():\n",
    "    \"\"\"Create a simplified ensemble of XGBoost and LightGBM\"\"\"\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=350,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=4,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=1.2  # Adjusted for class imbalance\n",
    "    )\n",
    "    lgbm = LGBMClassifier(\n",
    "        n_estimators=350,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=4,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        scale_pos_weight=1.2\n",
    "    )\n",
    "    return VotingClassifier(\n",
    "        estimators=[('xgb', xgb), ('lgbm', lgbm)],\n",
    "        voting='soft',\n",
    "        weights=[0.6, 0.4]  # Favor XGBoost slightly based on performance\n",
    "    ) -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Builds an ensemble model combining XGBoost and LightGBM using a soft voting strategy.\n",
    ". Parameters:\n",
    "\n",
    "XGBoost:\n",
    "n_estimators=350: Number of boosting rounds\n",
    "learning_rate=0.02: Step size for updates.\n",
    "max_depth=4: Maximum tree depth to control overfitting.\n",
    "subsample=0.85: Fraction of samples used per boosting round.\n",
    "colsample_bytree=0.8: Fraction of features used per tree.\n",
    "random_state=42: For reproducibility.\n",
    "eval_metric='logloss': Evaluation metric for optimization.\n",
    "scale_pos_weight=1.2: Adjusts for class imbalance (likely a slight imbalance in the dataset).\n",
    "\n",
    "LightGBM Model (lgbm):\n",
    "LGBMClassifier with similar parameters to XGBoost, plus:\n",
    "verbose=-1: Suppresses LightGBM logs.\n",
    "\n",
    "VotingClassifier:\n",
    "Combines xgb and lgbm using voting='soft' (averages predicted probabilities).\n",
    "weights=[0.6, 0.4]: Gives more weight to XGBoost based on expected performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!-- def optimize_threshold(y_true, y_prob):\n",
    "    \"\"\"Optimize classification threshold for maximum accuracy\"\"\"\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    best_threshold = 0.5\n",
    "    best_score = 0\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        score = accuracy_score(y_true, y_pred)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = threshold\n",
    "    return best_threshold -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Optimizes the classification threshold to maximize accuracy.\n",
    ". Tests thresholds from 0.1 to 0.9 (step size 0.01).\n",
    ". For each threshold, converts probabilities (y_prob) to binary predictions (y_pred) and computes accuracy against true labels (y_true).\n",
    ". Keeps the threshold with the highest accuracy.\n",
    ". Returns the best threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!-- def main():\n",
    "    # Load data\n",
    "    train_df, test_df = load_data()\n",
    "\n",
    "    # Preprocess data\n",
    "    X, y, X_test, test_ids, top_features = preprocess_data(train_df, test_df)\n",
    "\n",
    "    # Feature selection with RFE\n",
    "    base_model = XGBClassifier(random_state=42)\n",
    "    rfe = RFE(estimator=base_model, n_features_to_select=35)\n",
    "    X = pd.DataFrame(rfe.fit_transform(X, y), columns=X.columns[rfe.support_], index=X.index)\n",
    "    X_test = pd.DataFrame(rfe.transform(X_test), columns=X_test.columns[rfe.support_], index=X_test.index)\n",
    "\n",
    "    # Split data for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train ensemble model\n",
    "    print(\"Training ensemble model...\")\n",
    "    model = make_pipeline(\n",
    "        SimpleImputer(strategy='median'),\n",
    "        StandardScaler(),\n",
    "        build_ensemble_model()\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Validate model\n",
    "    val_probs = model.predict_proba(X_val)[:, 1]\n",
    "    threshold = optimize_threshold(y_val, val_probs)\n",
    "    val_preds = (val_probs >= threshold).astype(int)\n",
    "    val_score = accuracy_score(y_val, val_preds)\n",
    "    print(f\"Validation Accuracy: {val_score:.6f}\")\n",
    "\n",
    "    # Cross-validation with StratifiedKFold\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    print(f\"Cross-Validation Accuracy: {cv_scores.mean():.6f} (+/- {cv_scores.std() * 2:.6f})\")\n",
    "\n",
    "    # Generate predictions\n",
    "    test_probs = model.predict_proba(X_test)[:, 1]\n",
    "    test_preds = (test_probs >= threshold).astype(int)\n",
    "\n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({'ID': test_ids, 'Y': test_probs})\n",
    "    submission.to_csv('submission_fda_improved fda_31.csv', index=False)\n",
    "    print(\"\\nSubmission file 'submission_fda_improved fda_31.csv' has been created.\")\n",
    "    print(\"Submission head:\")\n",
    "    print(submission.head()) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function orchestrates the entire pipeline from data loading to submission file creation.\n",
    ". load the training and test datasets.\n",
    ". Preprocess Data to encode, optimize memory, and engineer features for both training and test sets.\n",
    ". Uses RFE with an XGBoost base model to select the top 35 features.\n",
    ". Updates X and X_test to include only the selected features.\n",
    ". Splits the training data into training (X_train, y_train) and validation (X_val, y_val) sets (80-20 split, random_state=42).\n",
    ". Creates a pipeline with imputation (median), scaling, and the ensemble model from build_ensemble_model() and fits the model on the training data.\n",
    ". Optimizes the threshold using optimize_threshold() and converts probabilities to binary predictions.\n",
    ". Performs 5-fold cross-validation with StratifiedKFold to ensure balanced folds and reports the mean cross-validation accuracy (0.997470) \n",
    ". Predicts probabilities on the test set and applies the optimized threshold to get binary predictions.\n",
    ". Creates submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!-- if __name__ == \"__main__\":\n",
    "    main() -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Calls the main() function to execute the entire pipeline."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
